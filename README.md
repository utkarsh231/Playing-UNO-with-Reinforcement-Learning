Uno-RL Agent

Uno-RL is a Deep Q-Learning based reinforcement learning agent trained to play the game of Uno from scratch using a custom-built environment and reward system.

The project demonstrates deep reinforcement learning applied to a multi-player, turn-based card game, using TensorFlow/Keras, experience replay, target networks, and Weights & Biases logging.

â¸»

Project Overview

	Details
Training time	109.5 hours
Frameworks	TensorFlow, Keras, Wandb, Numpy
RL Algorithm	Deep Q-Network (DQN)
Environment	Custom Uno Game Engine
Logging	Tensorboard + Weights & Biases

The agent was trained over 140k+ steps with experience replay, target network synchronization, and epsilon-greedy exploration.

â¸»

Key Components

Custom Uno Environment
	â€¢	Full Uno gameplay logic including:
	â€¢	Regular and special cards (Skip, Reverse, Draw Two, Wild, Wild Draw Four)
	â€¢	Turn-based system with reversing directions
	â€¢	Draw pile management
	â€¢	State representation:
	â€¢	Current top card one-hot encoding
	â€¢	Player hand card counts
	â€¢	Cards to draw information
	â€¢	Action space:
	â€¢	Play a specific card or draw a card

Reward Structure

Original rewards (first run):
	â€¢	Illegal move: -2
	â€¢	Drawing a card: -1
	â€¢	Playing a card: +2
	â€¢	Winning the game: +10

Updated reward structure (planned for second run, based on improvements):
	â€¢	Illegal move: -2
	â€¢	Drawing a card: -0.2
	â€¢	Playing a card: +1
	â€¢	Playing a special card: +2
	â€¢	Having 2 cards left: +2 (Near Win)
	â€¢	Having 1 card left: +5 (Almost Win)
	â€¢	Winning the game: +10

Model and Training
	â€¢	Q-network: MLP (Multi-layer perceptron) trained using MSE loss.
	â€¢	Replay Buffer: 10,000 most recent transitions.
	â€¢	Batch size: 512
	â€¢	Discount factor (gamma): 0.7
	â€¢	Learning rate: 1e-4
	â€¢	Target network updates: Every 20 steps.
	â€¢	Model saves: Every 20,000 steps.

Epsilon-greedy exploration
	â€¢	Initial epsilon: 1.0
	â€¢	Minimum epsilon: 0.05
	â€¢	Decay rate: 0.995

â¸»

Results

Cumulative Reward vs Steps

<img src="/mnt/data/cumulative_Reward.jpeg" width="800">


	â€¢	Cumulative reward stays mostly negative (around -6 on average), suggesting that while the agent learns to survive, winning is still rare under initial reward structure.

Epsilon Decay

<img src="/mnt/data/epsilon runtime.jpeg" width="800">


	â€¢	Smooth epsilon decay from 1.0 to about 0.94 over 140k steps, indicating continuous shift from exploration to exploitation.

Game Length

<img src="/mnt/data/game_length.jpeg" width="800">


	â€¢	The majority of games end in 4-5 moves, suggesting the agent either loses quickly or stabilizes fast.

Loss Curve

<img src="/mnt/data/loss.jpeg" width="800">


	â€¢	Loss is generally low but shows spikes around major replay memory flushes or after rare events.

Mean Reward

<img src="/mnt/data/mean_reward.jpeg" width="800">


	â€¢	Mean reward remains consistently negative (~-1) across most training steps in the initial run.

â¸»

Future Improvements (In Progress)

ðŸ”µ Reward Engineering:
Modified the reward structure to encourage strategic gameplay:
	â€¢	Positive rewards for nearing win conditions (2 or 1 cards left).
	â€¢	Additional reward for playing special cards (Skip, Draw 2, Wild, etc.).

ðŸ”µ Updated Training (second run):
	â€¢	A second run was conducted with the improved rewards.
	â€¢	Comparison of mean reward trends between the two runs:

<img src="/mnt/data/updated_mean_rewards.jpeg" width="800">


	â€¢	Early results show higher mean rewards with the new reward structure, suggesting better learning behavior.

â¸»

How to Run
	1.	Install dependencies

pip install -r requirements.txt

	2.	Train the agent

python train.py

	3.	Visualize logs on Tensorboard

tensorboard --logdir=logs/

	4.	Visualize full training metrics via Weights & Biases (optional, if wandb enabled).

â¸»

Folder Structure

Folder	Purpose
agent.py	Main DQN agent
environment.py	Custom Uno environment
train.py	Training loop
models/	Saved model checkpoints
logs/	TensorBoard logs
wandb/	Weights & Biases tracking (optional)



â¸»

Acknowledgements
	â€¢	Reinforcement learning concepts based on Sutton & Bartoâ€™s textbook.
	â€¢	TensorFlow/Keras documentation.
	â€¢	Uno card game rules adapted to fit a reinforcement learning context.

â¸»

âœ¨ Notes
	â€¢	Training took 109.5 hours over multiple sessions.
	â€¢	Code ensures model saving at consistent intervals to prevent losses after interruptions.
	â€¢	The environment is fully self-contained, no external Uno engine used.
